{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from gensim.models import Word2Vec\n",
    "from six import text_type as unicode\n",
    "from six import iteritems\n",
    "from six.moves import range\n",
    "import psutil\n",
    "from multiprocessing import cpu_count\n",
    "from os import path\n",
    "from time import time\n",
    "from glob import glob\n",
    "from six.moves import range, zip, zip_longest\n",
    "from six import iterkeys\n",
    "from collections import defaultdict, Iterable\n",
    "import random\n",
    "from random import shuffle\n",
    "from itertools import product,permutations\n",
    "from scipy.io import loadmat\n",
    "from scipy.sparse import issparse\n",
    "from io import open\n",
    "from os import path\n",
    "from time import time\n",
    "from multiprocessing import cpu_count\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import Counter\n",
    "from six.moves import zip\n",
    "os.system('pip install gensim')\n",
    "os.system('pip install psutil')\n",
    "# 服务器执行 单机跑不动\n",
    "\n",
    "\n",
    "\n",
    "p = psutil.Process(os.getpid())\n",
    "try:\n",
    "    p.set_cpu_affinity(list(range(cpu_count())))\n",
    "except AttributeError:\n",
    "    try:\n",
    "        p.cpu_affinity(list(range(cpu_count())))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "LOGFORMAT = \"%(asctime).19s %(levelname)s %(filename)s: %(lineno)s %(message)s\"\n",
    "\n",
    "\n",
    "def debug(type_, value, tb):\n",
    "    if hasattr(sys, 'ps1') or not sys.stderr.isatty():\n",
    "        sys.__excepthook__(type_, value, tb)\n",
    "    else:\n",
    "        import traceback\n",
    "        import pdb\n",
    "        traceback.print_exception(type_, value, tb)\n",
    "        print(u\"\\n\")\n",
    "        pdb.pm()\n",
    "\n",
    "# __author__ = \"Bryan Perozzi\"\n",
    "__email__ = \"bperozzi@cs.stonybrook.edu\"\n",
    "\n",
    "LOGFORMAT = \"%(asctime).19s %(levelname)s %(filename)s: %(lineno)s %(message)s\"\n",
    "\n",
    "class Graph(defaultdict):\n",
    "    \"\"\"Efficient basic implementation of nx `Graph' â€“ Undirected graphs with self loops\"\"\"  \n",
    "    def __init__(self):\n",
    "        super(Graph, self).__init__(list)\n",
    "\n",
    "    def nodes(self):\n",
    "        return self.keys()\n",
    "\n",
    "    def adjacency_iter(self):\n",
    "        return self.iteritems()\n",
    "\n",
    "    def subgraph(self, nodes={}):\n",
    "        subgraph = Graph()\n",
    "\n",
    "        for n in nodes:\n",
    "            if n in self:\n",
    "                subgraph[n] = [x for x in self[n] if x in nodes]\n",
    "\n",
    "        return subgraph\n",
    "\n",
    "    def make_undirected(self):\n",
    "\n",
    "        t0 = time()\n",
    "\n",
    "        for v in self.keys():\n",
    "            for other in self[v]:\n",
    "                if v != other:\n",
    "                    self[other].append(v)\n",
    "\n",
    "        t1 = time()\n",
    "\n",
    "        self.make_consistent()\n",
    "        return self\n",
    "\n",
    "    def make_consistent(self):\n",
    "        t0 = time()\n",
    "        for k in iterkeys(self):\n",
    "            self[k] = list(sorted(set(self[k])))\n",
    "\n",
    "        t1 = time()\n",
    "\n",
    "        self.remove_self_loops()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def remove_self_loops(self):\n",
    "\n",
    "        removed = 0\n",
    "        t0 = time()\n",
    "\n",
    "        for x in self:\n",
    "            if x in self[x]: \n",
    "                self[x].remove(x)\n",
    "                removed += 1\n",
    "\n",
    "        t1 = time()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def check_self_loops(self):\n",
    "        for x in self:\n",
    "            for y in self[x]:\n",
    "                if x == y:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def has_edge(self, v1, v2):\n",
    "        if v2 in self[v1] or v1 in self[v2]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def degree(self, nodes=None):\n",
    "        if isinstance(nodes, Iterable):\n",
    "            return {v:len(self[v]) for v in nodes}\n",
    "        else:\n",
    "            return len(self[nodes])\n",
    "\n",
    "    def order(self):\n",
    "    \"Returns the number of nodes in the graph\"\n",
    "        return len(self)    \n",
    "\n",
    "    def number_of_edges(self):\n",
    "    \"Returns the number of nodes in the graph\"\n",
    "        return sum([self.degree(x) for x in self.keys()])/2\n",
    "\n",
    "    def number_of_nodes(self):\n",
    "    \"Returns the number of nodes in the graph\"\n",
    "        return order()\n",
    "\n",
    "    def random_walk(self, path_length, alpha=0, rand=random.Random(), start=None):\n",
    "        \"\"\" Returns a truncated random walk.\n",
    "            path_length: Length of the random walk.\n",
    "            alpha: probability of restarts.\n",
    "            start: the start node of the random walk.\n",
    "        \"\"\"\n",
    "        G = self\n",
    "        if start:\n",
    "            path = [start]\n",
    "        else:\n",
    "        # Sampling is uniform w.r.t V, and not w.r.t E\n",
    "            path = [rand.choice(list(G.keys()))]\n",
    "\n",
    "        while len(path) < path_length:\n",
    "        cur = path[-1]\n",
    "        if len(G[cur]) > 0:\n",
    "            if rand.random() >= alpha:\n",
    "                path.append(rand.choice(G[cur]))\n",
    "            else:\n",
    "                path.append(path[0])\n",
    "        else:\n",
    "            break\n",
    "        return [str(node) for node in path]\n",
    "\n",
    "# TODO add build_walks in here\n",
    "\n",
    "def build_deepwalk_corpus(G, num_paths, path_length, alpha=0,\n",
    "                      rand=random.Random(0)):\n",
    "    walks = []\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    for cnt in range(num_paths):\n",
    "        rand.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(G.random_walk(path_length, rand=rand, alpha=alpha, start=node))\n",
    "\n",
    "    return walks\n",
    "\n",
    "def build_deepwalk_corpus_iter(G, num_paths, path_length, alpha=0,\n",
    "                      rand=random.Random(0)):\n",
    "    walks = []\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    for cnt in range(num_paths):\n",
    "        rand.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            yield G.random_walk(path_length, rand=rand, alpha=alpha, start=node)\n",
    "\n",
    "\n",
    "def clique(size):\n",
    "    return from_adjlist(permutations(range(1,size+1)))\n",
    "\n",
    "\n",
    "# http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks-in-python\n",
    "def grouper(n, iterable, padvalue=None):\n",
    "    \"grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')\"\n",
    "    return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)\n",
    "\n",
    "def parse_adjacencylist(f):\n",
    "    adjlist = []\n",
    "    for l in f:\n",
    "        if l and l[0] != \"#\":\n",
    "            introw = [int(x) for x in l.strip().split()]\n",
    "            row = [introw[0]]\n",
    "            row.extend(set(sorted(introw[1:])))\n",
    "            adjlist.extend([row])\n",
    "\n",
    "    return adjlist\n",
    "\n",
    "def parse_adjacencylist_unchecked(f):\n",
    "    adjlist = []\n",
    "    for l in f:\n",
    "        if l and l[0] != \"#\":\n",
    "            adjlist.extend([[int(x) for x in l.strip().split()]])\n",
    "\n",
    "    return adjlist\n",
    "\n",
    "def load_adjacencylist(file_, undirected=False, chunksize=10000, unchecked=True):\n",
    "\n",
    "    if unchecked:\n",
    "        parse_func = parse_adjacencylist_unchecked\n",
    "        convert_func = from_adjlist_unchecked\n",
    "    else:\n",
    "        parse_func = parse_adjacencylist\n",
    "        convert_func = from_adjlist\n",
    "\n",
    "    adjlist = []\n",
    "\n",
    "    t0 = time()\n",
    "\n",
    "    total = 0 \n",
    "    with open(file_) as f:\n",
    "        for idx, adj_chunk in enumerate(map(parse_func, grouper(int(chunksize), f))):\n",
    "            adjlist.extend(adj_chunk)\n",
    "            total += len(adj_chunk)\n",
    "    t1 = time()\n",
    "    t0 = time()\n",
    "    G = convert_func(adjlist)\n",
    "    t1 = time()\n",
    "\n",
    "\n",
    "    if undirected:\n",
    "        t0 = time()\n",
    "        G = G.make_undirected()\n",
    "        t1 = time()\n",
    "\n",
    "    return G \n",
    "\n",
    "\n",
    "def load_edgelist(file_, undirected=True):\n",
    "    G = Graph()\n",
    "    with open(file_) as f:\n",
    "        for l in f:\n",
    "            x, y = l.strip().split()[:2]\n",
    "            x = int(x)\n",
    "            y = int(y)\n",
    "            G[x].append(y)\n",
    "            if undirected:\n",
    "                G[y].append(x)\n",
    "\n",
    "    G.make_consistent()\n",
    "    return G\n",
    "\n",
    "\n",
    "def load_matfile(file_, variable_name=\"network\", undirected=True):\n",
    "    mat_varables = loadmat(file_)\n",
    "    mat_matrix = mat_varables[variable_name]\n",
    "\n",
    "    return from_numpy(mat_matrix, undirected)\n",
    "\n",
    "\n",
    "def from_networkx(G_input, undirected=True):\n",
    "    G = Graph()\n",
    "\n",
    "    for idx, x in enumerate(G_input.nodes_iter()):\n",
    "        for y in iterkeys(G_input[x]):\n",
    "            G[x].append(y)\n",
    "\n",
    "    if undirected:\n",
    "        G.make_undirected()\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def from_numpy(x, undirected=True):\n",
    "    G = Graph()\n",
    "\n",
    "    if issparse(x):\n",
    "        cx = x.tocoo()\n",
    "        for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "            G[i].append(j)\n",
    "    else:\n",
    "        raise Exception(\"Dense matrices not yet supported.\")\n",
    "\n",
    "    if undirected:\n",
    "        G.make_undirected()\n",
    "\n",
    "    G.make_consistent()\n",
    "    return G\n",
    "\n",
    "\n",
    "def from_adjlist(adjlist):\n",
    "    G = Graph()\n",
    "    \n",
    "    for row in adjlist:\n",
    "        node = row[0]\n",
    "        neighbors = row[1:]\n",
    "        G[node] = list(sorted(set(neighbors)))\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def from_adjlist_unchecked(adjlist):\n",
    "    G = Graph()\n",
    "    \n",
    "    for row in adjlist:\n",
    "        node = row[0]\n",
    "        neighbors = row[1:]\n",
    "        G[node] = neighbors\n",
    "\n",
    "    return G\n",
    "\n",
    "from collections import Counter, Mapping\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from six import string_types\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Vocab\n",
    "\n",
    "\n",
    "class Skipgram(Word2Vec):\n",
    "    \"\"\"A subclass to allow more customization of the Word2Vec internals.\"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary_counts=None, **kwargs):\n",
    "\n",
    "        self.vocabulary_counts = None\n",
    "\n",
    "        kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
    "        kwargs[\"workers\"] = kwargs.get(\"workers\", cpu_count())\n",
    "        kwargs[\"size\"] = kwargs.get(\"size\", 128)\n",
    "        kwargs[\"sentences\"] = kwargs.get(\"sentences\", None)\n",
    "        kwargs[\"window\"] = kwargs.get(\"window\", 10)\n",
    "        kwargs[\"sg\"] = 1\n",
    "        kwargs[\"hs\"] = 1\n",
    "\n",
    "        if vocabulary_counts != None:\n",
    "            self.vocabulary_counts = vocabulary_counts\n",
    "\n",
    "        super(Skipgram, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "__current_graph = None\n",
    "\n",
    "# speed up the string encoding\n",
    "__vertex2str = None\n",
    "\n",
    "def count_words(file):\n",
    "    \"\"\" Counts the word frequences in a list of sentences.\n",
    "  Note:\n",
    "    This is a helper function for parallel execution of `Vocabulary.from_text`\n",
    "    method.\n",
    "  \"\"\"\n",
    "    c = Counter()\n",
    "    with open(file, 'r') as f:\n",
    "        for l in f:\n",
    "            words = l.strip().split()\n",
    "            c.update(words)\n",
    "    return c\n",
    "\n",
    "\n",
    "def count_textfiles(files, workers=1):\n",
    "    c = Counter()\n",
    "    with ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "        for c_ in executor.map(count_words, files):\n",
    "            c.update(c_)\n",
    "    return c\n",
    "\n",
    "\n",
    "def count_lines(f):\n",
    "    if path.isfile(f):\n",
    "        num_lines = sum(1 for line in open(f))\n",
    "        return num_lines\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def _write_walks_to_disk(args):\n",
    "    num_paths, path_length, alpha, rand, f = args\n",
    "    G = __current_graph\n",
    "    t_0 = time()\n",
    "    with open(f, 'w') as fout:\n",
    "        for walk in graph.build_deepwalk_corpus_iter(G=G, num_paths=num_paths, path_length=path_length,\n",
    "                                             alpha=alpha, rand=rand):\n",
    "            fout.write(u\"{}\\n\".format(u\" \".join(v for v in walk)))\n",
    "    return f\n",
    "\n",
    "def write_walks_to_disk(G, filebase, num_paths, path_length, alpha=0, rand=random.Random(0), num_workers=cpu_count(),\n",
    "                        always_rebuild=True):\n",
    "    global __current_graph\n",
    "    __current_graph = G\n",
    "    files_list = [\"{}.{}\".format(filebase, str(x)) for x in list(range(num_paths))]\n",
    "    expected_size = len(G)\n",
    "    args_list = []\n",
    "    files = []\n",
    "\n",
    "    if num_paths <= num_workers:\n",
    "        paths_per_worker = [1 for x in range(num_paths)]\n",
    "    else:\n",
    "        paths_per_worker = [len(list(filter(lambda z: z!= None, [y for y in x])))\n",
    "                        for x in graph.grouper(int(num_paths / num_workers)+1, range(1, num_paths+1))]\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        for size, file_, ppw in zip(executor.map(count_lines, files_list), files_list, paths_per_worker):\n",
    "            if always_rebuild or size != (ppw*expected_size):\n",
    "                args_list.append((ppw, path_length, alpha, random.Random(rand.randint(0, 2**31)), file_))\n",
    "            else:\n",
    "                files.append(file_)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        for file_ in executor.map(_write_walks_to_disk, args_list):\n",
    "            files.append(file_)\n",
    "\n",
    "    return files\n",
    "\n",
    "class WalksCorpus(object):\n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "    def __iter__(self):\n",
    "        for file in self.file_list:\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    yield line.split()\n",
    "\n",
    "def combine_files_iter(file_list):\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                yield line.split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process(inputfile, output,representation_size = 64,window_size = 5 ,walk_length = 40,number_walks = 80,  undirected =True):\n",
    "    G = load_edgelist(inputfile, undirected=undirected)\n",
    "    print(\"Number of nodes: {}\".format(len(G.nodes())))\n",
    "\n",
    "    num_walks = len(G.nodes()) * number_walks\n",
    "\n",
    "    print(\"Number of walks: {}\".format(num_walks))\n",
    "\n",
    "    data_size = num_walks * walk_length\n",
    "\n",
    "    print(\"Data size (walks*length): {}\".format(data_size))\n",
    "\n",
    "    print(\"Walking...\")\n",
    "    walks = build_deepwalk_corpus(G, num_paths= number_walks,\n",
    "                                      path_length=walk_length, alpha=0, rand=random.Random(66))\n",
    "    print(\"Training...\")\n",
    "    model = Word2Vec(walks, size=representation_size, window= window_size, min_count=0, sg=1, hs=1, workers= 10)\n",
    "\n",
    "    model.wv.save_word2vec_format(output)\n",
    "\n",
    "def rundeepwalk():\n",
    "    cacheRoot = \"../cache/\"\n",
    "    inputfile = cacheRoot +  \"merchant_weighted_edglist_DeepWalk.txt\"\n",
    "    output = cacheRoot + \"merchant_weighted_edglist_DeepWalk.embeddings\" \n",
    "    # files = os.listdir(inputP)\n",
    "    process(inputfile, output)\n",
    "    \n",
    "\n",
    "rundeepwalk()\n",
    "# deepwalk --format edgelist  --input /usr/local/glsample/dep_wk_data/geo_code_edglist.txt \\\n",
    "# --max-memory-data-size 1319014400 --number-walks 80 --representation-size 36 --walk-length 40 --window-size 10 \\\n",
    "# --workers 8 --output /usr/local/glsample/geo_code_edglist.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('pip install node2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "import sys\n",
    "\n",
    "def emb_graph_2vec(inputpath,dim):\n",
    "    print(\"input name will be \",inputpath)\n",
    "    emb_name = inputpath.replace(\"weighted_edglist_filytypeTxt.edgelist\",\"\")\n",
    "    print(\"emb_name will be \",emb_name)\n",
    "\n",
    "    savename =inputpath.replace(\"weighted_edglist_filytypeTxt.edgelist\",\".emb\")\n",
    "    print(\"emb outfile name will be \",savename)\n",
    "    if os.path.exists(savename):\n",
    "        print(\"file alread exists in cache, please rename\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    graph = nx.read_edgelist(inputpath,create_using=nx.DiGraph())\n",
    "    # Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "    node2vec = Node2Vec(graph, dimensions=dim, walk_length=30, num_walks=200, workers=10) \n",
    "    # Embed nodes\n",
    "    print(\"training .... \")\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    print(\"training finished saving result... \")\n",
    "\n",
    "    print(\"saving %s file to disk \"%savename)\n",
    "    # Save embeddings for later use\n",
    "    model.wv.save_word2vec_format(savename)\n",
    "    print(\"done\")\n",
    "    # Save model for later use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input name will be  ./compFeatures/sourceEmb/mac1_weighted_edglist_filytypeTxt.edgelist\n",
      "emb_name will be  ./compFeatures/sourceEmb/mac1_\n",
      "emb outfile name will be  ./compFeatures/feature/mac1_.emb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|█| 132349/132349 [00:02<00:00, 48075.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training .... \n",
      "training finished saving result... \n",
      "saving ./compFeatures/feature/mac1_.emb file to disk \n",
      "done\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cacheRoot = \"../cache/\"\n",
    "inputpath = cacheRoot + \"mac1_weighted_edglist_filytypeTxt.edgelist\"\n",
    "try:\n",
    "    emb_graph_2vec(inputpath,36)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath =  cacheRoot +  \"merchant_weighted_edglist_filytypeTxt.edgelist\"\n",
    "try:\n",
    "    emb_graph_2vec(inputpath,64)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
